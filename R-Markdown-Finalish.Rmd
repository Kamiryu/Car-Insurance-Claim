---
title: "Car Insurance Claims"
author: "Fabien Roduit"
date: "2022-12-11"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


Libraries
```{r}
library(naniar)       # Missing values
library(ggplot2)      # Graphics
library(dplyr)
library(tidyr)
library(randomForest) # Random forest
library(corrplot)     # Plot correlations
library(knitr)         #Table presentation
library(GGally)       # Correlation and scatter plots
library(class)       # knn
library(rpart)       # trees
library(caret)       # preprocess data
library(rpart.plot)     # plot trees
library(forecast)       # accuracy

library(doParallel)#Accelerating computations by setting CPU to work in parallel
cores = detectCores()
cl <- makeCluster(cores-1)
registerDoParallel(cl)
```

```{r,warning=FALSE}
cars <- read.csv("train.csv", sep=",", header = TRUE)

str(cars)

cars <- cars[,c(-1)] # Drop ID

sapply(cars,function(x) length(unique(x))) # Check unique values

gg_miss_var(cars, show_pct = TRUE) # See if any missing value
```

```{r}
#Histograms
cars %>% select(1:3, 5:6,13,20:21,23,25:29,42:43) %>% gather() %>% 
 ggplot(aes(value)) + 
 facet_wrap(~ key, scales = "free") + 
 geom_histogram(color = "black", fill = "#6baed6") + 
 theme_minimal()


# Barcharts
cars %>% select(4,7:12,14:19,22,24,30:41) %>% gather() %>%
 ggplot(aes(x = value)) +
 facet_wrap(~ key, scales = "free") +
 geom_bar(color = "black", fill = "#6baed6") +
 theme_minimal()
```

```{r}
cars_numeric <- cars[,c(1:3, 5:6,13,20:21,23,25:29,42)] # Only numerical values

# Correlations
cor2 <- data.frame(round(cor(cars_numeric),3))
cor2

# Plot correlations
corrplot(as.matrix(cor2), order = 'AOE', type = 'lower', tl.pos = "lt",tl.cex = 0.5,)
corrplot(as.matrix(cor2), add = TRUE, type = 'upper', method = 'ellipse', order = 'AOE',
        diag = FALSE, tl.pos = 'n', cl.pos = 'n',tl.cex = 0.5)

# Dropping variables based on correlation
cars_reduced <- cars[,-c(20,13,23,25,29)]
```

```{r}
# Commented to avoid computing for the moment
# rf_fit <- randomForest(as.factor(cars$is_claim) ~ ., # Fit a random # forest with reduced set
#                        data = cars_reduced,
#                        ntree = 500,
#                        mtry = 4,
#                        importance = TRUE)
# 
# 
# rf_fit$importance[,3]
# 
# sort(round(rf_fit$importance[,4],3), decreasing = TRUE)
# 
# varImpPlot(rf_fit, type = 1)

cars_reduced_rf <- cars_reduced[,c("policy_tenure","age_of_car","age_of_policyholder",
                        "population_density","area_cluster","height",
                        "width","segment","model","length","engine_type",
                        "max_torque","max_power","ncap_rating","cylinder")]
```

Variable importance: <https://plos.figshare.com/articles/figure/Variable_importance_plot_mean_decrease_accuracy_and_mean_decrease_Gini_/12060105/1#>:\~:text=The%20mean%20decrease%20in%20Gini,the%20variable%20in%20the%20model.


```{r}
# Correlations
cor3 <- data.frame(round(cor(cars_reduced_rf[,c(1:4,6,7,10,14,15)]),3))
cor3

corrplot(as.matrix(cor3), order = 'AOE', type = 'lower', tl.pos = "lt",tl.cex = 0.5,)
corrplot(as.matrix(cor3), add = TRUE, type = 'upper', method = 'ellipse', order = 'AOE',
        diag = FALSE, tl.pos = 'n', cl.pos = 'n',tl.cex = 0.5)



# Filtering variables based on domain knowledge and correlations

cars_reduced <- cars_reduced_rf[,-c( 7,8,11,12,13,14,15)]
cars_final <- cbind(cars_reduced, cars$is_claim)
colnames(cars_final)[9] <- "is_claim"
```

```{r}
# We drop height and weight since model defines these features.

logistic_regression_final_variables <- glm(is_claim~., family=binomial(link='logit')
, data= cars_final[,-c(5,7)])

summary(logistic_regression_final_variables)

```


We can see that  the coefficient NA for cluster9 is symptomatic of a multicolinearity issue. Therefore, we remove area cluster and keep population instead.
```{r}
logistic_regression_final_variables <- glm(is_claim~., family=binomial(link='logit')
, data= cars_final[,-c(5,7,4)])

summary(logistic_regression_final_variables)
```

```{r}
cars_final <- cars_final[,-c(5,6,8)]
kable(head(cars_final), format="markdown")
```

Based on our previous analysis, our finals explanatory variables would be: policy_tenure, age_of_car, age_of_policyholder, model and population_density.


# EDA

```{r}
#Histograms
cars_final %>% select(1:4,6) %>% gather() %>% 
 ggplot(aes(value)) + 
 facet_wrap(~ key, scales = "free") + 
 geom_histogram(color = "black", fill = "#69b3a2") + 
  theme(plot.title=element_text(hjust=0.5),
        panel.background = element_rect(fill = "white"),
        panel.grid.major.y = element_line(color = "grey98")) 
```

```{r}
# Barcharts
cars_final %>% select(5)  %>% gather() %>%
 ggplot(aes(x = value)) +
 facet_wrap(~ key, scales = "free") +
 geom_bar(color = "black", fill = "#69b3a2") +
  theme(plot.title=element_text(hjust=0.5),
        panel.background = element_rect(fill = "white"),
        panel.grid.major.y = element_line(color = "grey98"))


# boxplot
cars_final %>% select(1:4)  %>% gather() %>%
 ggplot(aes(x = value,y="")) +
  
  facet_wrap(~ key, scales = "free")+
                   # add horizontal line to "whiskers" of boxplot
  geom_boxplot(fill = "#6baed6", width = 0.5) + 
  stat_boxplot(geom = "errorbar", width = 0.2) +# plot boxplot
  stat_summary(fun.y=mean, colour="darkred", geom="point", shape=18, size=3,show_guide = FALSE)+
  theme_classic() +
  theme(plot.title=element_text(hjust=0.5),
        panel.background = element_rect(fill = "white"),
        panel.grid.major.y = element_line(color = "grey98")) 


```

We can see that the distribution of the data is not symmetrical. However, since the data is already normalized, we are not able to do a log transform on the data except for population density.

```{r}
cars_final$population_density <- log(cars_final$population_density)

normalized.pop <- (cars_final$population_density - min (cars_final$population_density)) / (max(cars_final$population_density)-min(cars_final$population_density))

cars_final$population_density <- as.vector(normalized.pop)
```

```{r}
# boxplot
cars_final %>% select(1:4)  %>% gather() %>%
 ggplot(aes(x = value,y="")) +
  
  facet_wrap(~ key, scales = "free")+
                   # add horizontal line to "whiskers" of boxplot
  geom_boxplot(fill = "#6baed6", width = 0.5) + 
  stat_boxplot(geom = "errorbar", width = 0.2) +# plot boxplot
  stat_summary(fun.y=mean, colour="darkred", geom="point", shape=18, size=3,show_guide = FALSE)+
  theme_classic() +
  theme(plot.title=element_text(hjust=0.5),
        panel.background = element_rect(fill = "white"),
        panel.grid.major.y = element_line(color = "grey98")) 
```

With the log transformation on population_density, the distribution has become a little bit more centered and symmetrical.

```{r}
kable(summary(cars_final), format="markdown")
```

```{r}
ggpairs(cars_final[,c(1:4,6)])+
  theme_bw()
```

We can see from the plots above that our remaining variables are not highly correlated between them.



```{r}
proportions(table(cars_final$is_claim))

set.seed(1)
index <- sample(nrow(cars_final),nrow(cars_final)*0.60)
cars_train = cars_final[index,]
cars_validation = cars_final[-index,]

proportions(table(cars_train$is_claim))
proportions(table(cars_validation$is_claim))

```

```{r}
# Add dummy variables for Education
cars_final$model <- as.factor(cars_final$model)                       # change variable format to factor 

dummies <- dummyVars(~ ., data = cars_final)                             # create object for dummy variables
cars_final_dummy <- as.data.frame(predict(dummies, newdata = cars_final))      # apply dummies to data

cars_final$is_claim <- as.factor(cars_final$is_claim)                # change variable format to factor
cars_final_dummy$is_claim <- as.factor(cars_final_dummy$is_claim) 

head(cars_final_dummy)                                                  # resulting in 11
str(cars_final_dummy)

```

```{r}

train_dummy <- cars_final_dummy[index,]
valid_dummy <- cars_final_dummy[-index,]

dim(train_dummy)
dim(valid_dummy)
proportions(table(cars_train$is_claim))
proportions(table(cars_validation$is_claim))
proportions(table(train_dummy$is_claim))
proportions(table(valid_dummy$is_claim))
```


## KNN
```{r}
# Find optimal k

# Data frame for k from 1 to 50 and respective accuracy
accuracy <- data.frame(k = seq(1, 50, 1), overallaccuracy = rep(0, 50))  

# Use for loop to find k with highest accuracy
for (i in 1:50) {
  knn.pred<-knn(train_dummy[,-16],valid_dummy[,-16],
                cl=train_dummy[,16],k=i)
  accuracy[i,2]<-confusionMatrix(knn.pred,valid_dummy[,16])$overall[1]
} # loop for calculating the accuracy fo k=30
# Display accuracy for ten first k
kable(accuracy[1:15,], row.names = F, caption = "Accuracy")
```

```{r}
which(accuracy[,2] == max(accuracy[,2]))   # max accuracy
```

```{r}
# Plot accuracy for different k
ggplot(accuracy, aes(k, overallaccuracy)) + 
  geom_line() + 
  theme_minimal() +
  labs (title = "K nearest neighbours: Overall accuracy vs K", 
        y = "Accuracy", 
        x = "k nearest neigbours" )
```

```{r}
knn.pred.valid <- knn(train_dummy[, -16], valid_dummy[, -16], cl = train_dummy[, 16], k = 3 ,prob = T)     #prediction on validation data usig k=3
cmk <- confusionMatrix(knn.pred.valid,valid_dummy[,16])    #confusion matrix 
fourfoldplot(cmk$table, color = c("cyan", "pink"),
             conf.level = 0, margin = 1, main = "Confusion Matrix for KNN")     # plot confusion ma
```

## Classification tree

```{r}
# Classification tree

pred_tree_tr <- rpart(cars_train$is_claim ~.,
                   data = cars_train, # Data set
                   method = "class") # Method: Classification

rpart.plot(pred_tree_tr) # Plot the tree

pred_tree_tr_proba <- predict(pred_tree_tr, cars_train[,c(-6)])[,2] # Take the columns for success prediction

pred_tree_tr_outcome <- ifelse(pred_tree_tr_proba >= 0.5,1,0) # Use cutoff value at 0.5 to determine success



pred_tree_v_proba <- predict(pred_tree_tr, cars_validation[,c(-6)])[,2]
# Take the columns for success prediction

pred_tree_v_outcome <- ifelse(pred_tree_v_proba >= 0.5,1,0) # Use cutoff value at 0.5 to determine success
```

```{r}
error_train <- round(accuracy(
  pred_tree_tr_outcome,
  cars_train$is_claim),# Actual price
  3) # Rounded digit 

error_validation <- round(accuracy(
  pred_tree_v_outcome,# Fitted value
  cars_validation$is_claim),
  3) # Actual state 


# RMSE 
rmse_tree<- data.frame(
  rbind(error_train[,1:3],
        error_validation)[,1:3]) # Combine RMS errors into a data frame

rownames(rmse_tree) <- c("Train", "Validation") # Rename columns

rmse_tree = kable(rmse_tree,  caption = "Error table for tree classification")


rmse_tree # Display RMSE
```
```{r}
# Confusion matrix for classification tree
conf_matrix_tree <- confusionMatrix(
  factor(pred_tree_v_outcome),
  factor(cars_validation$is_claim),
  positive = "1")

# plot confusion matrix
fourfoldplot(conf_matrix_tree$table,
             color = c("cyan", "pink"),
             conf.level = 0,
             margin = 1,
             main = "Confusion Matrix for KNN") 
```





