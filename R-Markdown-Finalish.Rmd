---
title: "Car Insurance Claims"
author: "The miners"
date: "2022-12-11"
output:
  html_document:
    theme: paper
    highlight: tango
    toc: true
    toc_float: true
    toc_collapsed: true
    toc_depth: 4
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Libraries

```{r}
library(naniar)       # Missing values
library(ggplot2)      # Graphics
library(dplyr)
library(tidyr)
library(randomForest) # Random forest
library(corrplot)     # Plot correlations
library(knitr)         #Table presentation
library(GGally)       # Correlation and scatter plots
library(class)       # knn
library(rpart)       # trees
library(caret)       # preprocess data
library(rpart.plot)     # plot trees
library(forecast)       # accuracy
library(neuralnet)    # Neural netwok
library(CustomerScoringMetrics) #Lift and gain
library(data.table)




source("PlotResidLogist.R") # Deviance Residuals
library(doParallel)#Accelerating computations by setting CPU to work in parallel
cores = detectCores()
cl <- makeCluster(cores-1)
registerDoParallel(cl)
```

# Introduction

```{r,warning=FALSE}
cars <- read.csv("train.csv", sep=",", header = TRUE)

str(cars)

cars <- cars[,c(-1)] # Drop ID

sapply(cars,function(x) length(unique(x))) # Check unique values

gg_miss_var(cars, show_pct = TRUE) # See if any missing value
```

```{r}
#Histograms
cars %>% select(1:3, 5:6,13,20:21,23,25:29,42:43) %>% gather() %>% 
 ggplot(aes(value)) + 
 facet_wrap(~ key, scales = "free") + 
 geom_histogram(color = "black", fill = "#6baed6") + 
 theme_minimal()


# Barcharts
cars %>% select(4,7:12,14:19,22,24,30:41) %>% gather() %>%
 ggplot(aes(x = value)) +
 facet_wrap(~ key, scales = "free") +
 geom_bar(color = "black", fill = "#6baed6") +
 theme_minimal()
```

```{r}
cars_numeric <- cars[,c(1:3, 5:6,13,20:21,23,25:29,42)] # Only numerical values

# Correlations
cor2 <- data.frame(round(cor(cars_numeric),3))
cor2

# Plot correlations
corrplot(as.matrix(cor2), order = 'AOE', type = 'lower', tl.pos = "lt",tl.cex = 0.5,)
corrplot(as.matrix(cor2), add = TRUE, type = 'upper', method = 'ellipse', order = 'AOE',
        diag = FALSE, tl.pos = 'n', cl.pos = 'n',tl.cex = 0.5)

# Dropping variables based on correlation
cars_reduced <- cars[,-c(20,13,23,25,29)]
```

```{r}
# Commented to avoid computing for the moment
# rf_fit <- randomForest(as.factor(cars$is_claim) ~ ., # Fit a random # forest with reduced set
#                        data = cars_reduced,
#                        ntree = 500,
#                        mtry = 4,
#                        importance = TRUE)
# 
# 
# rf_fit$importance[,3]
# 
# sort(round(rf_fit$importance[,4],3), decreasing = TRUE)
# 
# varImpPlot(rf_fit, type = 1)

cars_reduced_rf <- cars_reduced[,c("policy_tenure","age_of_car","age_of_policyholder",
                        "population_density","area_cluster","height",
                        "width","segment","model","length","engine_type",
                        "max_torque","max_power","ncap_rating","cylinder")]
```

Variable importance: <https://plos.figshare.com/articles/figure/Variable_importance_plot_mean_decrease_accuracy_and_mean_decrease_Gini_/12060105/1#>:\~:text=The%20mean%20decrease%20in%20Gini,the%20variable%20in%20the%20model.

```{r}
# Correlations
cor3 <- data.frame(round(cor(cars_reduced_rf[,c(1:4,6,7,10,14,15)]),3))
cor3

corrplot(as.matrix(cor3), order = 'AOE', type = 'lower', tl.pos = "lt",tl.cex = 0.5,)
corrplot(as.matrix(cor3), add = TRUE, type = 'upper', method = 'ellipse', order = 'AOE',
        diag = FALSE, tl.pos = 'n', cl.pos = 'n',tl.cex = 0.5)



# Filtering variables based on domain knowledge and correlations

cars_reduced <- cars_reduced_rf[,-c( 7,8,11,12,13,14,15)]
cars_final <- cbind(cars_reduced, cars$is_claim)
colnames(cars_final)[9] <- "is_claim"
```

```{r}
# We drop height and weight since model defines these features.

logistic_regression_final_variables <- glm(is_claim~., family=binomial(link='logit')
, data= cars_final[,-c(5,7)])

summary(logistic_regression_final_variables)

```

We can see that the coefficient NA for cluster9 is symptomatic of a multicolinearity issue. Therefore, we remove area cluster and keep population instead.

```{r}
logistic_regression_final_variables <- glm(is_claim~., family=binomial(link='logit')
, data= cars_final[,-c(5,7,4)])

summary(logistic_regression_final_variables)
```

```{r}
cars_final <- cars_final[,-c(5,6,8)]
kable(head(cars_final), format="markdown")
```

Based on our previous analysis, our finals explanatory variables would be: policy_tenure, age_of_car, age_of_policyholder, model and population_density.

# EDA

```{r}
#Histograms
cars_final %>% select(1:4,6) %>% gather() %>% 
 ggplot(aes(value)) + 
 facet_wrap(~ key, scales = "free") + 
 geom_histogram(color = "black", fill = "#6baed6") + 
  theme(plot.title=element_text(hjust=0.5),
        panel.background = element_rect(fill = "white"),
        panel.grid.major.y = element_line(color = "grey98")) 
```

```{r}
# Barcharts
cars_final %>% select(5)  %>% gather() %>%
 ggplot(aes(x = value)) +
 facet_wrap(~ key, scales = "free") +
 geom_bar(color = "black", fill = "#6baed6") +
  theme(plot.title=element_text(hjust=0.5),
        panel.background = element_rect(fill = "white"),
        panel.grid.major.y = element_line(color = "grey98"))


# boxplot
cars_final %>% select(1:4)  %>% gather() %>%
 ggplot(aes(x = value,y="")) +
  
  facet_wrap(~ key, scales = "free")+
                   # add horizontal line to "whiskers" of boxplot
  geom_boxplot(fill = "#6baed6", width = 0.5) + 
  stat_boxplot(geom = "errorbar", width = 0.2) +# plot boxplot
  stat_summary(fun.y=mean, colour="darkred", geom="point", shape=18, size=3,show_guide = FALSE)+
  theme_classic() +
  theme(plot.title=element_text(hjust=0.5),
        panel.background = element_rect(fill = "white"),
        panel.grid.major.y = element_line(color = "grey98")) 


```

We can see that the distribution of the data is not symmetrical. However, since the data is already normalized, we are not able to do a log transform on the data except for population density.

```{r}
# Adjust scale with log for population density
cars_final$population_density <- log(cars_final$population_density)

# Normalizing population_density
normalized.pop <- (cars_final$population_density - min (cars_final$population_density)) / (max(cars_final$population_density)-min(cars_final$population_density))

cars_final$population_density <- as.vector(normalized.pop)


# Normalizing policy tenure
normalized.policy <- (cars_final$policy_tenure - min (cars_final$policy_tenure)) / (max(cars_final$policy_tenure)-min(cars_final$policy_tenure))

cars_final$policy_tenure <- as.vector(normalized.policy)
```

```{r}
# boxplot
cars_final %>% select(1:4)  %>% gather() %>%
 ggplot(aes(x = value,y="")) +
  
  facet_wrap(~ key, scales = "free")+
                   # add horizontal line to "whiskers" of boxplot
  geom_boxplot(fill = "#6baed6", width = 0.5) + 
  stat_boxplot(geom = "errorbar", width = 0.2) +# plot boxplot
  stat_summary(fun.y=mean, colour="darkred", geom="point", shape=18, size=3,show_guide = FALSE)+
  theme_classic() +
  theme(plot.title=element_text(hjust=0.5),
        panel.background = element_rect(fill = "white"),
        panel.grid.major.y = element_line(color = "grey98")) 
```

With the log transformation on population_density, the distribution has become a little bit more centered and symmetrical.

```{r}
kable(summary(cars_final), format="markdown")
```

```{r}
# Commented for time computing
# ggpairs(cars_final[,c(1:4,6)])+
#   theme_bw()
```

We can see from the plots above that our remaining variables are not highly correlated between them.

```{r}
#boxplot for the distribution of 0 1 
df = cars_final[, -5] 
df$is_claim <- as.factor(df$is_claim)
library(reshape2)
df.melt = melt(df)
# HEAD

df.boxplot = ggplot(df.melt, aes(y=value,is_claim)) + 
  geom_boxplot() + 
  facet_wrap(~variable, scales = "free", ncol=3) + theme_classic() + xlab("Variables")

df.boxplot

```

```{r}
barplot(prop.table(table(cars$is_claim)),
        col = rainbow(2),
        ylim = c(0, 1),
        main = "Class Distribution") # outcome variable distribution 
prop.table(table(cars$is_claim))
```

When the proportions of the different classes in a classification problem are imbalanced (in our case 1 present about 6% from is_claim), it means that one class (the majority class) is much more frequent than the other class(es) (the minority class(es)). This can cause the model to have a high performance (e.g. high F1-score) on the majority class and low performance on the minority class.

To address this issue, we can re-sample the data to create a more balanced distribution of the classes. This can be done using techniques such as undersampling (removing examples from the majority class) or oversampling . These techniques can help the model to better learn from the minority class and improve its performance on it.

```{r}

set.seed(1)
index <- sample(nrow(cars_final),nrow(cars_final)*0.60)
cars_train = cars_final[index,]
cars_validation = cars_final[-index,]

proportions(table(cars_train$is_claim))
proportions(table(cars_validation$is_claim))

```

# Predictive analytics

## Classification tree

### Method's description

A classification tree is a method of supervised algorithm used for classification. It consists in continuously splitting the data into sub-parts based on data features (sub-parts form rectangles on the graph) until getting only one class in the splitted area. At that point, data's class is the most homogeneous as said previously. The splits are done in such a way that it minimizes the impurity of the new area. The algorithm may chose one of the following measures of impurity: Gini Index or Entropy measure. These measures ranges between 0 and 1 and allow to identify to which class the data belongs.

*Intuition:* A greater number of terminal nodes is expected to decrease the overall error until reaching the point of overfitting.

Therefore, it is wise to stop tree's growth. In that sense, we need to prune the tree and use cross-validation to get a best tree size.

### Method application

```{r}
# Classification tree

tree_fit <- rpart(cars_train$is_claim ~.,
                  data = cars_train, # Data set
                  cp = 0.001,
                  method = "class") # Method: Classification

rpart.plot(tree_fit)

# Predict values for train set
pred_tree_fit <- predict(tree_fit,
                         cars_train[,c(-6)],
                         type ="class")


confusionMatrix(pred_tree_fit,
  factor(cars_train$is_claim),
  positive = "1")
```

Plotting the classification tree with the trained data results in only 1 leaf. Also, by looking at the confusion matrix, we notice that model predicted every observation as is_claim = 0. Such an output is expected because the unbalance of the outcome variable. So, let's try again with a downsampled train dataset.

```{r}
# Downsample 
cars_train[,6] <- as.factor(cars_train$is_claim)
train_downsampling <- downSample(cars_train[,-6],cars_train$is_claim,yname = "is_claim")

tree_fit <- rpart(train_downsampling$is_claim ~.,
                  data = train_downsampling, # Data set
                  cp = 0.001,
                  method = "class") # Method: Classification


rpart.plot(tree_fit) # Plot the resulting tree



# Predict values for validation set
pred_tree_v_tr <- predict(tree_fit,
                         cars_validation[,c(-6)],
                         type ="class")




# Confusion matrix for classification tree with validation set
conf_matrix_tree_v <- confusionMatrix(
  pred_tree_v_tr,
  factor(cars_validation$is_claim),
  positive = "1")

# plot confusion matrix
fourfoldplot(conf_matrix_tree_v$table,
             color = c("cyan", "pink"),
             conf.level = 0,
             margin = 1,
             main = "Confusion Matrix for Classification
             Tree for validation set") 
```

We see that using a balanced data set solved our issue and display a full grown tree.

```{r}
# Find the best cp with minimal xerror ||  /!\ code may need to be improved
cp_minimal <- tree_fit$cptable[which.min(
  tree_fit$cptable[,"CP"]), # Take lowest cp and Take lowest xerror
  1]# CP column 

pruned.tree <- prune(tree_fit, cp= cp_minimal) # Prune the train model with lowest cp
```

We try to look at the lowest cp value to prune the tree. The found value equals 0.001. The value is similar to the default value that we chosen to compute to fit the classification tree.

It is known that we should not choose a tree based on it's cp, but we should choose rather a tree size. Ordinarily, we would like to have a small tree with a small cp. Such a tree would avoid overfitting.

```{r}
# Cross-validation

table_cp <- pruned.tree$cptable # Save cps

# DOES SOMEONE HAVE A BETTER METHOD ?
rows_small_errors_std <- head(order(table_cp[,5]),6) # Know the rows (folds) having  the smallest standard errors of the estimates

table_error <- matrix(table_cp[,4] + table_cp[,5]) # Sum the std error and the error

table_error <- cbind(table_cp[,1], # Include CP
                     table_cp[,2], # Include nsplit
                     table_error,  # Error + std error
                     table_cp[,5]) # Std error

rows_small_errors_std <- head(sort(table_error[,4]),6) # Know the rows (folds) having  the smallest std error

rows_small_errors_std # Display and chose the best size

rows_small_errors <- head(sort(table_error[,3]),6) # Display the smallest standard errors
rows_small_errors # Display

kable(table_cp)

best_cp <- table_cp[4,1]
```

To choose a tree size we should look for the smallest minimum error within one std. error, also we want a small CP value.

We see that the fourth observation where the nsplit = 3 and CP = 0.005 gives the best results.

```{r}
pruned.tree <- prune(tree_fit, cp = best_cp) # Prune the train model with best cp

rpart.plot(pruned.tree) # Plot the tree with best size
```

### Performance evaluation

```{r}
# Prediction on Validation data
pred_tree_v_pruned <- predict(pruned.tree, # Pruned tree
                         cars_validation[,c(-16)],# Validation set
                         type ="class")

# Confusion matrix for classification tree with validation set
conf_matrix_tree_v <- confusionMatrix(
  pred_tree_v_pruned,
  factor(cars_validation$is_claim),
  positive = "1")
conf_matrix_tree_v
# plot confusion matrix
fourfoldplot(conf_matrix_tree_v$table,
             color = c("cyan", "pink"),
             conf.level = 0,
             margin = 1,
             main = "Confusion Matrix for Classification
             Tree (best size) for validation set") 
```

### Conclusion on method's performance

# Lift Chart

```{r}
#Getting the probabilities from the model
tree.outcome <- as.data.frame(predict(pruned.tree, # Pruned tree
                         cars_validation[,c(-16)],# Validation set
                         type ="prob"))[,2]


#Lift chart plot 
liftChart(tree.outcome,  factor(cars_validation$is_claim))
```

```{r}
topDecileLift(tree.outcome,  factor(cars_validation$is_claim))
```

## KNN

```{r}
# Add dummy variables 
cars_final$model <- as.factor(cars_final$model)                       # change variable format to factor 

dummies <- dummyVars(~ ., data = cars_final)                             # create object for dummy variables
cars_final_dummy <- as.data.frame(predict(dummies, newdata = cars_final))      # apply dummies to data

cars_final$is_claim <- as.factor(cars_final$is_claim)                # change variable format to factor
cars_final_dummy$is_claim <- as.factor(cars_final_dummy$is_claim) 

head(cars_final_dummy)                                                  # resulting in 11
str(cars_final_dummy)

train_dummy <- cars_final_dummy[index,] # training set with dummies  
valid_dummy <- cars_final_dummy[-index,] #validation set with dummies
train_new <- downSample(train_dummy[,-16],train_dummy$is_claim,yname = "is_claim") # undersampling for having same probation 
str(train_new)
proportions(table(train_new$is_claim))

summary(train_new$is_claim)

```

```{r}
set.seed(1)
# Find optimal k

# Data frame for k from 1 to 50 and respective accuracy
accuracy <- data.frame(k = seq(1, 50, 1), overallaccuracy = rep(0, 50))  

# Use for loop to find k with highest accuracy
for (i in 1:50) {
  knn.pred<-knn(train_new[,-16],valid_dummy[,-16],
                cl=train_new[,16],k=i)
  accuracy[i,2]<-confusionMatrix(knn.pred,valid_dummy[,16])$overall[1]
} 
# Data frame for k from 1 to 50 and respective specificity
Specificity <-  data.frame(k = seq(1, 50, 1), overallspecificity = rep(0, 50))

# Use for loop to find k with highest specificity
for (i in 1:50) {
  knn.pred<-knn(train_new[,-16],valid_dummy[,-16],
                cl=train_new[,16],k=i)
  Specificity[i,2]<-confusionMatrix(knn.pred,valid_dummy[,16])$byClass[2]
} 


```

```{r}
which(accuracy[,2] == max(accuracy[,2]))   # max accuracy
which(Specificity[,2] == max(Specificity[,2]))   # max specificity 

```

```{r}
# Plot accuracy for different k
ggplot(accuracy, aes(k, overallaccuracy)) + 
  geom_line() + 
  theme_minimal() +
  labs (title = "K nearest neighbours: Overall accuracy vs K", 
        y = "Accuracy", 
        x = "k nearest neigbours" )

# Plot specificity for different k
ggplot(Specificity, aes(k, overallspecificity)) + 
  geom_line() + 
  theme_minimal() +
  labs (title = "K nearest neighbours: Overall specificity vs K", 
        y = "Specificity", 
        x = "k nearest neigbours" )


```

Since we are looking fo k which give us max optimum value of Specificity we chose k = 49

```{r}
knn.pred.valid <- knn(train_new[, -16], valid_dummy[, -16], cl = train_new[, 16], k = 49 ,prob = T)     #prediction on validation data using best k=6
cmk <- confusionMatrix(knn.pred.valid,valid_dummy[,16])    #confusion matrix 
cmk$byClass[2]
fourfoldplot(cmk$table, color = c("cyan", "pink"),
             conf.level = 0, margin = 1, main = "Confusion Matrix for KNN")     # plot confusion ma
cmk
```

Lift Chart

```{r}
knn_outcome <- as.data.table(cbind(as.data.table(knn.pred.valid), knn_prob = attr(knn.pred.valid, "prob")))

# Adjusting probabilities
knn_outcome[knn.pred.valid != 1, knn_prob := 1 - knn_prob]

# liftChart(knn_outcome[,2], valid_dummy[,16])
```

```{r}
#topDecileLift()
```

## Neural Network

### Method descript: how the method(algo) works. (Explain like in an interview)

Neural network tries to identify complex relationships between variables. It can be represented by edges (weights) interconnecting nodes (values) and organized in different layers. There are three levels of layer:

1.  *Input layer:* concerned with the predictors

2.  *Hidden layer:* concerned with weighted relations

3.  *Output layer:* concerned with the final output (class)

intuition: Take input values and compute weights

Values from variables go into the input layer. Then we initialize a very small and random weight in relation with our imputed values to start training the model. The resulting value is a node in the hidden layer. Weights are then updated again for many times in order to find an optimum value which minimize the output error (predicted output with respect to the true output). These weights depend on a function called "transfer function". A common practice is to  apply the weights updating by starting with the output layer and turning back to find the input values. Such a model is called back propagation and we used it for prediction.

### Data transformation if needed
```{r}
summary(train_new)
```


### Method application

Common practice is to use 1 hidden layer for fitting neural networks.

#### 1 Hidden layer and 2 nodes

```{r}
# run Neural Network (N.N.) with 1 hidden layer and 2 nodes
nn_1H_2N <- neuralnet(is_claim ~ .,
                    data = train_new,
                    hidden = 2,
                    algorithm = "backprop") # 1 hidden layer of 2 nodes

plot(nn_1H_2N, rep="best")



# Predict the output on validation set
validation_prediction_1H_2N <- predict(nn_1H_2N,
                                      valid_dummy[,-16],
                                      type = "class")

validation_prediction_1H_2N_binary <-
  ifelse(validation_prediction_1H_2N[,1]
         >= 0.5, 0, 1) # Transform probabilities as binary outcome 



# Confusion matrix for classification tree with validation set
conf_matrix_1H2N_valid <- confusionMatrix(
  factor(validation_prediction_1H_2N_binary),
  factor(valid_dummy$is_claim), # change dataset
  positive = "1")

# plot confusion matrix
fourfoldplot(conf_matrix_1H2N_valid$table,
             color = c("cyan", "pink"),
             conf.level = 0,
             margin = 1,
             main = "Confusion Matrix for Neural Network
             1 hidden layer of 2 nodes for validation set") 
```

#### 1 Hidden layer and 3 nodes

```{r}
# run Neural Network (N.N.) with 1 hidden layer and 3 nodes
nn_1H_3N <- neuralnet(is_claim ~.,
                    data = train_new,
                    hidden = 3, # 1 hidden layer of 3 nodes
                    algorithm = "backprop")

plot(nn_1H_3N, rep="best")



# Predict the output on validation set
validation_prediction_1H_3N <- predict(nn_1H_3N,
                                      valid_dummy[,-16],
                                      type = "class")

validation_prediction_1H_3N_binary <-
  ifelse(validation_prediction_1H_3N[,1]
         >= 0.5, 0, 1) # Transform probabilities as binary outcome 
```

### Performance evaluation

```{r}
# Confusion matrix for classification tree with validation set
conf_matrix_1H3N_valid <- confusionMatrix(
  factor(validation_prediction_1H_3N_binary),
  factor(valid_dummy$is_claim), # change dataset
  positive = "1")

# plot confusion matrix
fourfoldplot(conf_matrix_1H3N_valid$table,
             color = c("cyan", "pink"),
             conf.level = 0,
             margin = 1,
             main = "Confusion Matrix for Neural Network
             1 hidden layer of 3 nodes for validation set") 
```

### Conclusion on method performance

## Logistic Regression

The logistic regression is a model that is also suited for binary response variables. <br> Building the model with a logit link function as it is usually the best suited one for the logistic regression

```{r}
cars.lg <- glm(is_claim ~., data= train_downsampling, family=binomial(link="logit"))

summary(cars.lg) # Displaying the resulting model
```

From the dummies born from "model" variable, none of them are statistically significant. H0 cannot be rejected for the beta estimate of population_density either. The other explanatory variables are significant. <br>

```{r}
PlotResidLogist(cars.lg) # Deviance residuals
```

If we plot the deviance residuals, we notice that the positive residuals and negative ones each form a clear group.<br> The residuals appearing only on one half of the observations is due to downsampling. The way every observation is tightly grouped may result from the artificially generated data. It is also the case for the pearson residuals. We can see an extreme residual that stands out (observation 440). Considering the amount of observations, this single observation should not affect the model too much. A robust logisitc regression is not required.

```{r}
anova(cars.lg, test="Chisq")
```

According to the analysis of the deviance table, "population_density" and "model" could be unworth to be part of the model with p-value\>0.05. <br> To further investigate the importance of the variables, we can run a stepwise selection.

```{r}
cars_glm_back <- step(cars.lg, direction = "backward") # Model with backward selection
summary(cars_glm_back)
```

The backward selection results in a model with 3 explanatory variables: policy_tenure, age_of_car and age_of_policyholder. Every beta is very stastically significant.

```{r}
cars_glm_forward <- step(cars.lg, direction = "forward") # Model with forward selection
summary(cars_glm_forward)
```

The forward selection ends up with different variables: the model is the same as the full one. This selection results in a minority of statistically significant beta estimates.

```{r}
cars_glm_both <- step(cars.lg, direction = "both") # Forward and backward
summary(cars_glm_both)
```

The selection with forward and backward at the same time results in the same variables as the backward selection. Every coefficient is also significant. The AIC of the reduced model is marginally better.

Confusion Matrix

```{r}
# Full model
pred_logi <- predict(cars.lg, newdata = cars_validation[,-16], type = "response")

# 50% cutoff
logi_outcome <- as.data.table(pred_logi)
logi_outcome <- ifelse(logi_outcome > 0.5, 1, 0)


#Confusion Matrix
cm1 <- confusionMatrix(
  factor(logi_outcome),
  factor(cars_validation$is_claim), 
  positive = "1")
#Plot
fourfoldplot(cm1$table,
             color = c("cyan", "pink"),
             conf.level = 0,
             margin = 1,
             main = "Confusion Matrix for the full logistic regression") 
```

The downsampling allows us to better detect when a claim occurs, but it is as tbe cost of increasing the false positives which are numerous as we can see. We can try to choose another cutoff to see if we can improve the model.

```{r}
# cutoff at probability of 0.55
logi_outcome <- as.data.table(pred_logi)
logi_outcome <- ifelse(logi_outcome > 0.55, 1, 0)

#Confusion Matrix
cm2 <- confusionMatrix(
  factor(logi_outcome),
  factor(cars_validation$is_claim), 
  positive = "1")
#Plot
fourfoldplot(cm2$table,
             color = c("cyan", "pink"),
             conf.level = 0,
             margin = 1,
             main = "Confusion Matrix for the full logistic regression") 
```

We can see the tradeoff right away. The 5% increased on the cutoff value reduced the predictions of is_claim by about 30% for both true positives and false positives.

```{r}
# Backward/Both 
cars.lg2 <- glm(is_claim ~ policy_tenure+age_of_car+age_of_policyholder , data= train_downsampling, family=binomial(link="logit"))
pred_logi_back <- predict(cars.lg2, newdata = cars_validation[,-16], type = "response")

# 50% cutoff
logi_outcome_back <- as.data.table(pred_logi_back)
logi_outcome_back <- ifelse(logi_outcome_back > 0.5, 1, 0)

#Confusion Matrix
cm3 <- confusionMatrix(
  factor(logi_outcome_back),
  factor(cars_validation$is_claim), 
  positive = "1")
#Plot
fourfoldplot(cm3$table,
             color = c("cyan", "pink"),
             conf.level = 0,
             margin = 1,
             main = "Confusion Matrix for the reduced logistic regression") 
```

With the model with a reduced number of variables, we seem to get a little bit more false positives true positive: more is_claim= 1 predictions. The prediction performance is very similar. Thus, to reduce computational resources, we can choose the model with the reduced number of variables since they perform similarly. <br> <br> We can try to see if the model can perform better with the whole training set combined with a very low cutoff value.

```{r}
# Backward/Both 
cars.lg3 <- glm(is_claim ~ policy_tenure+age_of_car+age_of_policyholder , data= cars_train, family=binomial(link="logit"))
pred_logi_back2 <- predict(cars.lg3, newdata = cars_validation[,-16], type = "response")

# 50% cutoff
logi_outcome_back2 <- as.data.table(pred_logi_back2)
logi_outcome_back2 <- ifelse(logi_outcome_back > 0.5, 1, 0)

#Confusion Matrix
cm4 <- confusionMatrix(
  factor(logi_outcome_back2),
  factor(cars_validation$is_claim), 
  positive = "1")
#Plot
fourfoldplot(cm4$table,
             color = c("cyan", "pink"),
             conf.level = 0,
             margin = 1,
             main = "Confusion Matrix for the reduced logistic regression") 
```

Ploting with selected variables

```{r}

```

Lift Chart

```{r}
logi_outcome <- as.data.table(pred_logi)
logi_outcome <- cbind(ifelse(logi_outcome > 0.5, 1, 0), logi_outcome)

```

```{r}
t <-  glm(is_claim ~ ., data= train_downsampling, family=binomial(link="logit"))
summary(t)
```

data mining is the best next test

# All methods comparison

```{r}
Accuracy <-  data.frame(LR = confusionMatrix(
  factor(logi_outcome_back2),
  factor(cars_validation$is_claim), 
  positive = "1")$overall[1])  #predict accuracy by using logistic regression 
Accuracy <- cbind(Accuracy, KNN = confusionMatrix(knn.pred.valid,valid_dummy[,16])$overall[1])      #predict accuracy by using KNN
Accuracy <- cbind(Accuracy,CT = confusionMatrix(
  pred_tree_v_pruned,
  factor(cars_validation$is_claim),
  positive = "1")$overall[1])    #predict accuracy by using tree
Accuracy <- cbind(Accuracy,NN=confusionMatrix(
  factor(validation_prediction_1H_2N_binary),
  factor(valid_dummy$is_claim), # change dataset
  positive = "1")$overall[1])     #predict accuracy by using neural network
Accuracy.melt = melt(Accuracy)
ggplot(data = Accuracy.melt,aes(x=reorder(variable, value),y=value))+ylab("Accuracy") + xlab("Method") +
  geom_bar(stat="identity", fill="steelblue")+
  geom_text(aes(label=value), vjust=1.6, color="white", size=3.5)+
  theme_minimal()
```

```{r}
Specificity_P <-  data.frame(LR = confusionMatrix(
  factor(logi_outcome_back2),
  factor(cars_validation$is_claim), 
  positive = "1")$byClass[2])  #predict specificity by using logistic regression 
Specificity_P <- cbind(Specificity_P, KNN = confusionMatrix(knn.pred.valid,valid_dummy[,16])$byClass[2])      #predict accuracy by using KNN
Specificity_P <- cbind(Specificity_P,CT = confusionMatrix(
  pred_tree_v_pruned,
  factor(cars_validation$is_claim),
  positive = "1")$byClass[2])    #predicts pecificity by using tree
Specificity_P <- cbind(Specificity_P,NN=confusionMatrix(
  factor(validation_prediction_1H_2N_binary),
  factor(valid_dummy$is_claim), # change
  positive = "1")$byClass[2])     #predict specificity by using neural network
Specificity_P.melt = melt(Specificity_P)
ggplot(data = Specificity_P.melt,aes(x=reorder(variable, value),y=value))+ylab("Specificity") + xlab("Method") +
  geom_bar(stat="identity", fill="steelblue")+
  geom_text(aes(label=value), vjust=1.6, color="white", size=3.5)+
  theme_minimal()

```

# Conclusions
