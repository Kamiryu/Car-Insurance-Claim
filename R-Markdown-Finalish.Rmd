---
title: "Car Insurance Claims"
author: "The miners"
date: "2022-12-11"
output:
  pdf_document:
    toc: yes
    toc_depth: '4'
  html_document:
    theme: paper
    highlight: tango
    toc: yes
    toc_float: yes
    toc_collapsed: yes
    toc_depth: 4
---

```{r setup, include=FALSE,warning=FALSE,cache=TRUE}
knitr::opts_chunk$set(echo = FALSE,warning=FALSE,cache=TRUE)
```

```{r}
# Libraries
library(naniar)       # Missing values
library(ggplot2)      # Graphics
library(dplyr)
library(tidyr)
library(randomForest) # Random forest
library(corrplot)     # Plot correlations
library(knitr)        #Table presentation
library(GGally)       # Correlation and scatter plots
library(class)        # knn
library(rpart)        # trees
library(caret)        # preprocess data
library(rpart.plot)   # plot trees
library(forecast)     # accuracy
library(neuralnet)    # Neural netwok
library(CustomerScoringMetrics) #Lift and gain
library(data.table)

source("PlotResidLogist.R") # Deviance Residuals

library(doParallel)#Accelerating computations by setting CPU to work in parallel
cores = detectCores()
cl <- makeCluster(cores-1)
registerDoParallel(cl)
```

# Introduction

Being able to predict the risk of a policyholder's complaint is fundamental for an insurance company. It could impact its profitability. We decided to conduct an analysis about that industry to better understand and identify the complaint phenomenon with regards to a car insurance business. Specifically, we want to predict whether an insured is going to claim a file. We will try to achieve that through different algorithmic classification methods and by trying to identify the best classification model for such a situation.

A dataset containing information about insurance holders and details about them is going to be used to achive our goal. The dataset is large and comes from the Kaggle web platform. It contains 58'500 observations and 44 variables. Among this information, we find attributes such as, the population density of the insured's city, the age of the policyholder, the age of the insured car, the car model, etc. However, we are particularly interested in the explained variable "Is_claim" It is a boolean indicator showing if a policyholder filed a claim. A positive case will be denoted by 1 and 0 in the opposite case. Therefore we will try to predict it.

To conduct our research, we plan to undertake the following steps. First, we will have a first look at the data and drop some variables, because our data set is very large and requires a lot of computation power. We will continue with a regular exploratory data analysis to understand our data. Then, we will explore different methods of classification. After this step, We are going to explore classification tree, k-nearest neighbors, neural network, logistic regression and ensembles. We will finish by comparing these method to identify the most suitable for our case.


### Dataset preparation

We need to manipulate our data before starting the Exploratory Data Analysis part in order to reduce the amount of data and making our computers ables to handle the dataset. 


```{r,warning=FALSE}
cars <- read.csv("train.csv", sep=",", header = TRUE)# Original data set

str(cars) # types of variable and dimensions

cars <- cars[,c(-1)] # Drop ID

sapply(cars,function(x) length(unique(x))) # Check unique values

gg_miss_var(cars, show_pct = TRUE) # See if any missing value
```

Our original data set is precisely composed of 58592 observations and 44 variables. Variables are numerical and categorical where some of them are booleans. We notice the presence of variables that might be less relevant. We could drop them in order to alleviate computation resources. Let's start with the variable "ID" which is of no help.

```{r warning=FALSE}
#Histograms
cars %>% select(1:3, 5:6,13,20:21,23,25:29,42:43) %>% gather() %>% 
 ggplot(aes(value)) + 
 facet_wrap(~ key, scales = "free") + # Display figures in many facets
 geom_histogram(color = "black", fill = "#6baed6") + 
 theme_minimal()


# Barcharts
cars %>% select(4,7:12,14:19,22,24,30:41) %>% gather() %>%
 ggplot(aes(x = value)) +
 facet_wrap(~ key, scales = "free") + # Display figures in many facets
 geom_bar(color = "black", fill = "#6baed6") +
 theme_minimal()
```

```{r}
cars_numeric <- cars[,c(1:3,5:6,13,20:21,23,25:29,42)] # Only numerical values

# Correlations
cor2 <- data.frame(round(cor(cars_numeric),3)) # Table with correlations
cor2

# Plot correlations
corrplot(as.matrix(cor2),  # Plot of upper right part
         order = 'AOE',
         type = 'lower',
         tl.pos = "lt",
         tl.cex = 0.5,)

corrplot(as.matrix(cor2), # Plot of lower left part
         add = TRUE, type = 'upper',
         method = 'ellipse',
         order = 'AOE',
         diag = FALSE,
         tl.pos = 'n',
         cl.pos = 'n',
         tl.cex = 0.5)

# Dropping variables based on correlation
cars_reduced <- cars[,-c(20,13,23,25,29)]
```

Looking at correlations we note a high correlation for the variable displacement such as cylinder (0.87), turning_radius with length (0.95), gross_weight with length (0.86), width with length (0.92).

displacement, airbags, gear_box, turning_radius, gross_weight

```{r}
#Random forest
# rf_fit <- randomForest(as.factor(cars$is_claim) ~ .,#Fit
#                         data = cars_reduced,
#                         ntree = 500, 
#                         mtry = 4, # variables randomly sampled as candidates at each # split. 
#                         importance = TRUE)
#  
# varImpPlot(rf_fit, type = 1) # Variable importance
# sort(round(rf_fit$importance[,4],3), decreasing = TRUE)[1:19]

# Variables keeped by the random forest
cars_reduced_rf <- cars_reduced[,c("policy_tenure","age_of_car",
                                   "age_of_policyholder","population_density",
                                   "area_cluster","height","width","segment",
                                   "model","length","engine_type","max_torque",
                                   "max_power","ncap_rating","cylinder")]
```

To select important variables we also use a random forest method which sort variables by decrease in Gini score. The ranking showing the important variables recommends at least these 15 variables: policy_tenure, age_of_car, age_of_policyholder, population_density, area_cluster, height, width, segment, model, length, engine_type, max_torque, max_power, ncap_rating and cylinder.

```{r}
# Correlations
cor3 <- data.frame(round(cor(cars_reduced_rf[,c(1:4,6,7,10,14,15)]),3))
cor3

corrplot(as.matrix(cor3), # Plot of lower left part
         order = 'AOE',
         type = 'lower',
         tl.pos = "lt",
         tl.cex = 0.5,)

corrplot(as.matrix(cor3), add = TRUE, # Plot of upper right part
         type = 'upper',
         method = 'ellipse',
         order = 'AOE',
         diag = FALSE,
         tl.pos = 'n',
         cl.pos = 'n',
         tl.cex = 0.5)



# Filtering variables based on domain knowledge and correlations
cars_reduced <- cars_reduced_rf[,-c( 7,8,11,12,13,14,15)]# Drop variables

cars_final <- cbind(cars_reduced, cars$is_claim) # Bind explanatory variable

colnames(cars_final)[9] <- "is_claim" # Rename column
```

We drop the variables width, segment, engine_type, max_torque, max_power, ncap_rating and rating.

## CHECK code

```{r}
# We drop height and weight since model defines these features.

logistic_regression_final_variables <- glm(is_claim~.,
                                           family=binomial(link='logit'),
                                           data= cars_final[,-c(5,7)])

summary(logistic_regression_final_variables)

```

We can see that the coefficient NA for cluster9 is symptomatic of a multicolinearity issue. Therefore, we remove area cluster and keep population instead.

```{r}
logistic_regression_final_variables <- glm(is_claim~.,
                                           family=binomial(link='logit')
                                           , data= cars_final[,-c(5,7,4)])

summary(logistic_regression_final_variables)
```

```{r}
cars_final <- cars_final[,-c(5,6,8)]
kable(head(cars_final), format="markdown")
```

Based on our previous analysis, our finals explanatory variables would be: policy_tenure, age_of_car, age_of_policyholder, model and population_density.

# EDA

```{r}
#Histograms
cars_final %>% select(1:4,6) %>% gather() %>% 
 ggplot(aes(value)) + 
 facet_wrap(~ key, scales = "free") + 
 geom_histogram(color = "black", fill = "#6baed6") + 
  theme(plot.title=element_text(hjust=0.5),
        panel.background = element_rect(fill = "white"),
        panel.grid.major.y = element_line(color = "grey98")) 
```

```{r}
# Barcharts
cars_final %>% select(5)  %>% gather() %>%
 ggplot(aes(x = value)) +
 facet_wrap(~ key, scales = "free") +
 geom_bar(color = "black", fill = "#6baed6") +
  theme(plot.title=element_text(hjust=0.5),
        panel.background = element_rect(fill = "white"),
        panel.grid.major.y = element_line(color = "grey98"))


# boxplot
cars_final %>% select(1:4)  %>% gather() %>%
 ggplot(aes(x = value,y="")) +
  
  facet_wrap(~ key, scales = "free")+
                   # add horizontal line to "whiskers" of boxplot
  geom_boxplot(fill = "#6baed6", width = 0.5) + 
  stat_boxplot(geom = "errorbar", width = 0.2) +# plot boxplot
  stat_summary(fun.y=mean, colour="darkred", geom="point", shape=18, size=3,show_guide = FALSE)+
  theme_classic() +
  theme(plot.title=element_text(hjust=0.5),
        panel.background = element_rect(fill = "white"),
        panel.grid.major.y = element_line(color = "grey98")) 


```

We can see that the distribution of the data is not symmetrical. However, since the data is already normalized, we are not able to do a log transform on the data except for population density.

```{r}
# Adjust scale with log for population density
cars_final$population_density <- log(cars_final$population_density)

# Normalizing population_density
normalized.pop <- (cars_final$population_density - min (cars_final$population_density)) / (max(cars_final$population_density)-min(cars_final$population_density))

cars_final$population_density <- as.vector(normalized.pop)


# Normalizing policy tenure
normalized.policy <- (cars_final$policy_tenure - min (cars_final$policy_tenure)) / (max(cars_final$policy_tenure)-min(cars_final$policy_tenure))

cars_final$policy_tenure <- as.vector(normalized.policy)
```

```{r}
# boxplot
cars_final %>% select(1:4)  %>% gather() %>%
 ggplot(aes(x = value,y="")) +
  
  facet_wrap(~ key, scales = "free")+
                   # add horizontal line to "whiskers" of boxplot
  geom_boxplot(fill = "#6baed6", width = 0.5) + 
  stat_boxplot(geom = "errorbar", width = 0.2) +# plot boxplot
  stat_summary(fun.y=mean, colour="darkred", geom="point", shape=18, size=3,show_guide = FALSE)+
  theme_classic() +
  theme(plot.title=element_text(hjust=0.5),
        panel.background = element_rect(fill = "white"),
        panel.grid.major.y = element_line(color = "grey98")) 
```

With the log transformation on population_density, the distribution has become a little bit more centered and symmetrical.

```{r}
kable(summary(cars_final), format="markdown")
```

```{r}
# Commented for time computing
# ggpairs(cars_final[,c(1:4,6)])+
#   theme_bw()
```

We can see from the plots above that our remaining variables are not highly correlated between them.

```{r}
#boxplot for the distribution of 0 1 
df = cars_final[, -5] 
df$is_claim <- as.factor(df$is_claim)
library(reshape2)
df.melt = melt(df)
# HEAD

df.boxplot = ggplot(df.melt, aes(y=value,is_claim)) + 
  geom_boxplot() + 
  facet_wrap(~variable, scales = "free", ncol=3) + theme_classic() + xlab("Variables")

df.boxplot

```
We can can the distribution is normal for is_claim = 0 and is_claim = 1 with outliers 
```{r}
barplot(prop.table(table(cars$is_claim)),
        col = rainbow(2),
        ylim = c(0, 1),
        main = "Class Distribution") # outcome variable distribution 
prop.table(table(cars$is_claim))
```

When the proportions of the different classes in a classification problem are imbalanced (in our case 1 present about 6% from is_claim), it means that one class (the majority class) is much more frequent than the other class(es) (the minority class(es)). This can cause the model to have a high performance (e.g. high F1-score) on the majority class and low performance on the minority class.

To address this issue, we can re-sample the data to create a more balanced distribution of the classes. This can be done using techniques such as undersampling (removing observations from the majority class) or oversampling . These techniques can help the model to learn better from the minority class and improve its performance on it.

```{r}
set.seed(1)
index <- sample(nrow(cars_final),nrow(cars_final)*0.60)
cars_train = cars_final[index,]
cars_validation = cars_final[-index,]
proportions(table(cars_train$is_claim))
proportions(table(cars_validation$is_claim))

# Downsample 
cars_train[,6] <- as.factor(cars_train$is_claim)
train_downsampling <- downSample(cars_train[,-6],cars_train$is_claim,yname = "is_claim")

# Normalizing function
normalizing <- function(x) {
    (x - min(x)) / (max(x) - min(x))
}

# Normalizing variables again on downsample's basis:

# Normalizing train data - downsampled
train_downsampling[,c(1:4)] <- lapply(train_downsampling[,c(1:4)], normalizing)

# Normalizing validation data
cars_validation[,c(1:4)] <- lapply(cars_validation[,c(1:4)], normalizing)


summary(cars_validation)
```

# Predictive analytics

## Classification tree

### Method's description

A classification tree is a method of supervised algorithm used for classification. It consists in continuously splitting the data into sub-parts based on data features (sub-parts could be segmented in rectangular areas on the graph) until getting only one class in the splitted area. At that point, data's class is the most homogeneous as said previously. The splits are done in such a way that it minimizes the impurity of the new area. The algorithm may chose one of the following measures of impurity: Gini Index or Entropy measure. These measures range between 0 and 1 and allow to identify to which class the data belongs.

*Intuition*: A higher number of terminal nodes is expected to decrease the overall error until reaching the point of overfitting.

Therefore, it is wise to stop a tree's growth. In that sense, we need to prune the tree and use cross-validation to get a best tree size.

### Method application

```{r}
tree_fit <- rpart(train_downsampling$is_claim ~.,
                  data = train_downsampling, # Data set
                  cp = 0.001,
                  method = "class") # Method: Classification


rpart.plot(tree_fit) # Plot the resulting tree



# Predict values for validation set
pred_tree_v_tr <- predict(tree_fit,
                         cars_validation[,c(-6)],
                         type ="class")
```
We start by fitting a full grown tree. It allows us to get a first look.

```{r}
# Find the best cp with minimal xerror ||  /!\ code may need to be improved
cp_minimal <- tree_fit$cptable[which.min(
  tree_fit$cptable[,"CP"]), # Take lowest cp and Take lowest xerror
  1]# CP column 

pruned.tree <- prune(tree_fit, cp= cp_minimal) # Prune the train model with lowest cp
```

We want to identify the lowest cp value to prune the tree. That value equals 0.001 and is similar to the default value that we chosed to compute the full grown classification tree.

Also, it is known that we should not choose only a tree based on it's cp, but we should also consider the tree size. Ordinarily, we would like to have a small tree with a small cp. Such a tree would avoid overfitting. To choose a tree size we should find the smallest minimum error within one standard error and a small CP value.

```{r}
# Cross-validation

table_cp <- pruned.tree$cptable # Save cps

# DOES SOMEONE HAVE A BETTER METHOD ?
rows_small_errors_std <- head(order(table_cp[,5]),6) # Know the rows (folds) having  the smallest standard errors of the estimates

table_error <- matrix(table_cp[,4] + table_cp[,5]) # Sum the std error and the error

table_error <- cbind(table_cp[,1], # Include CP
                     table_cp[,2], # Include nsplit
                     table_error,  # Error + std error
                     table_cp[,5]) # Std error

rows_small_errors_std <- head(sort(table_error[,4]),6) # Know the rows (folds) having  the smallest std error
rows_small_errors <- head(sort(table_error[,3]),6) # Display the smallest standard errors

rows_small_errors_std # Display and chose the best size
rows_small_errors # Display

kable(table_cp)

best_cp <- table_cp[as.integer(names(rows_small_errors)[1]),1]
```


We see that the 16th observation where nsplit = 35 and CP = 0.0014509 gives the best error choice.

```{r}
pruned.tree <- prune(tree_fit, cp = best_cp) # Prune the train model with best cp

rpart.plot(pruned.tree) # Plot the tree with best size

sum(pruned.tree$frame$ncompete == 0) # Number of leaves
```

The resulting pruned tree at its best size has 36 leaves.

### Performance evaluation

```{r}
# Confusion matrix for classification tree with validation set
conf_matrix_tree_v <- confusionMatrix(
  pred_tree_v_tr,
  factor(cars_validation$is_claim),
  positive = "1")
conf_matrix_tree_v
# plot confusion matrix
fourfoldplot(conf_matrix_tree_v$table,
             color = c("cyan", "pink"),
             conf.level = 0,
             margin = 1,
             main = "Confusion Matrix for Classification
             Tree for validation set") 
```

Looking at the performance metrics for the full grown tree:

-   Accuracy highs at 43.2%

-   Specificity highs at 41%

-   Sensitivity is 74.5%

We see  that the tree is better at classifying negative than positives outcome and struggles with false positive values that represents the majority of classifieds values.
```{r}
# Prediction on Validation data
pred_tree_v_pruned <- predict(pruned.tree, # Pruned tree
                         cars_validation[,c(-16)],# Validation set
                         type ="class")

# Confusion matrix for classification tree with validation set
conf_matrix_tree_v <- confusionMatrix(
  pred_tree_v_pruned,
  factor(cars_validation$is_claim),
  positive = "1")
conf_matrix_tree_v

# plot confusion matrix
fourfoldplot(conf_matrix_tree_v$table,
             color = c("cyan", "pink"),
             conf.level = 0,
             margin = 1,
             main = "Confusion Matrix for Classification
             Tree (best size) for validation set") 

```

Looking at the performance metrics for the best size tree:

-   Accuracy highs at 47.2 %

-   Specificity highs at 45.6 %

-   Sensitivity is 71.6%

From all positive outcome predicted, almost 8.3% where right while almost 95.9% of negative outcomes where correctly predicted. Also, the performances for accuracy improved of almost 4% and the one for specificity improved of almost 5%, but the performance was lower for sensitivity of almost 3%.


### Conclusion on method's performance

We conclude that predicting positive outcomes is difficult for the classification tree model since not even 10% of its positive predictions are right, even if cross-validation has been used.
The method is better at predicting negative outcomes than positive ones. Therefore, we may want to analyse an other model with hopping to find better results with respect to positive prediction.

#### Lift Chart

```{r}
#Getting the probabilities from the model
tree.outcome <- as.data.frame(predict(pruned.tree, # Pruned tree
                         cars_validation[,c(-16)],# Validation set
                         type ="prob"))[,2]


#Lift chart plot 
liftChart(tree.outcome,  factor(cars_validation$is_claim))
```

```{r}
topDecileLift(tree.outcome,  factor(cars_validation$is_claim))
```

## KNN

### Method's description

The k-nearest-neighbors algorithm that can be used for classification. To classify or predict a new record, the method relies on finding "similar" records in the training data. These "neighbors" are then used to derive a classification for the new record by voting (for classification) .

### Data transformation

```{r}
# Add dummy variables 
cars_final$model <- as.factor(cars_final$model)                       # change variable format to factor 

dummies <- dummyVars(~ ., data = cars_final)                             # create object for dummy variables
cars_final_dummy <- as.data.frame(predict(dummies, newdata = cars_final))      # apply dummies to data

cars_final$is_claim <- as.factor(cars_final$is_claim)                # change variable format to factor
cars_final_dummy$is_claim <- as.factor(cars_final_dummy$is_claim) 

head(cars_final_dummy)                                                  # resulting in 11
str(cars_final_dummy)


train_dummy <- cars_final_dummy[index,] # training set with dummies  
valid_dummy <- cars_final_dummy[-index,] #validation set with dummies
train_dummy_downsampled <- downSample(train_dummy[,-16],train_dummy$is_claim,yname = "is_claim") # undersampling for having same probation 


# Normalizing data
train_dummy_downsampled[,c(1:4)] <- lapply(train_dummy_downsampled[,c(1:4)], normalizing)

valid_dummy[,c(1:4)] <- lapply(valid_dummy[,c(1:4)], normalizing)
```

### Method application

```{r}
set.seed(1)
# Find optimal k

# Data frame for k from 1 to 50 and respective accuracy
accuracy <- data.frame(k = seq(1, 50, 1), overallaccuracy = rep(0, 50))  

# Use for loop to find k with highest accuracy
for (i in 1:50) {
  knn.pred<-knn(train_dummy_downsampled[,-16],valid_dummy[,-16],
                cl=train_dummy_downsampled[,16],k=i)
  accuracy[i,2]<-confusionMatrix(knn.pred,valid_dummy[,16])$overall[1]
} 
# Data frame for k from 1 to 50 and respective specificity
Specificity <-  data.frame(k = seq(1, 50, 1), overallspecificity = rep(0, 50))

# Use for loop to find k with highest specificity
for (i in 1:50) {
  knn.pred<-knn(train_dummy_downsampled[,-16],valid_dummy[,-16],
                cl=train_dummy_downsampled[,16],k=i)
  Specificity[i,2]<-confusionMatrix(knn.pred,valid_dummy[,16])$byClass[2]
} 
# Data frame for k from 1 to 50 and respective Sensitivity 
Sensitivity  <-  data.frame(k = seq(1, 50, 1), overallSensitivity  = rep(0, 50))

# Use for loop to find k with highest Sensitivity 
for (i in 1:50) {
  knn.pred<-knn(train_dummy_downsampled[,-16],valid_dummy[,-16],
                cl=train_dummy_downsampled[,16],k=i)
  Sensitivity[i,2]<-confusionMatrix(knn.pred,valid_dummy[,16])$byClass[1]
} 



```

```{r}
which(accuracy[,2] == max(accuracy[,2]))   # max accuracy
which(Specificity[,2] == max(Specificity[,2]))   # max specificity 
which(Sensitivity[,2] == max(Sensitivity[,2]))   # max specificity 
```

```{r}
# Plot accuracy for different k
ggplot(accuracy, aes(k, overallaccuracy)) + 
  geom_line() + 
  theme_minimal() +
  labs (title = "K nearest neighbours: Overall accuracy vs K", 
        y = "Accuracy", 
        x = "k nearest neigbours" )

# Plot Sensitivity for different k
ggplot(Sensitivity, aes(k, overallSensitivity)) + 
  geom_line() + 
  theme_minimal() +
  labs (title = "K nearest neighbours: Overall Sensitivity vs K", 
        y = "Sensitivity", 
        x = "k nearest neigbours" )

# Plot specificity for different k
ggplot(Specificity, aes(k, overallspecificity)) + 
  geom_line() + 
  theme_minimal() +
  labs (title = "K nearest neighbours: Overall specificity vs K", 
        y = "Specificity", 
        x = "k nearest neigbours" )


```

Since we are looking for k which give us max optimum value of Specificity we chose k = 31

```{r}
knn.pred.valid <- knn(train_dummy_downsampled[, -16], valid_dummy[, -16], cl = train_dummy_downsampled[, 16], k = 35 ,prob = T)     #prediction on validation data using best k=6
cmk <- confusionMatrix(knn.pred.valid,valid_dummy[,16])    #confusion matrix 
cmk$byClass[2]
fourfoldplot(cmk$table, color = c("cyan", "pink"),
             conf.level = 0, margin = 1, main = "Confusion Matrix for KNN")     # plot confusion ma
cmk
```

Lift Chart

```{r}
#Extracting the probabilities
knn_outcome <- as.data.table(cbind(as.data.table(knn.pred.valid), knn_prob = attr(knn.pred.valid, "prob")))

# Adjusting probabilities
knn_outcome[knn.pred.valid != 1, knn_prob := 1 - knn_prob]

# plot
liftChart(knn_outcome$knn_prob, factor(valid_dummy$is_claim))
```

```{r}
topDecileLift(knn_outcome$knn_prob, factor(valid_dummy$is_claim))
```

With a top decile lift of 1.40, the model's lift drastically decreases at the second decile then slowly declines with each decile.

## Neural Network
### Method description:

Neural network tries to identify complex relationships between variables. It can be represented by edges (weights) interconnecting nodes (values) and organized in different layers. There are three levels of layer:

1.  *Input layer:* concerned with the predictors

2.  *Hidden layer:* concerned with weighted relations

3.  *Output layer:* concerned with the final output (class)

*Intuition*: Transform input values and identify relations (translated by weights) to identify output values.

Values from variables go into the input layer. Then we initialize a very small and random weight in relation with our imputed values to start training the model. The resulting value is a node in the hidden layer. Weights are then updated again for many times in order to find an optimum value which minimize the output error (predicted output with respect to the true output). These weights depend on a function called "transfer function".

### Method application

Common practice is to use 1 hidden layer for fitting neural networks. We are first going to try the algorithm with two nodes. Next, we are going to continue the analysis with a third node in order to compare performances.

#### 1 hidden layer and 2 nodes

```{r}
# Run Neural Network (N.N.) with 1 hidden layer and 2 nodes
nn_1H_2N <- neuralnet(is_claim ~ .,
                    data = train_dummy_downsampled,
                    hidden = 2) # 1 hidden layer of 2 nodes

plot(nn_1H_2N, rep="best")



# Predict the output on validation set
validation_prediction_1H_2N <- predict(nn_1H_2N,
                                      valid_dummy[,-16],
                                      type = "class")

validation_prediction_1H_2N_binary <-
  ifelse(validation_prediction_1H_2N[,1]
         >= 0.5, 0, 1) # Transform probabilities as binary outcome 
```
We use an algorithm of neural network with one layer of two nodes. The variables that are used as input need to be numerical and normalized (from 0 to 1). These steps has already been done previously and this is why we input the variable model under its "dummified" form. Then, we train the algorithm and use it to predict data from our validation set.

#### 1 hidden layer and 4 nodes
```{r}
# run Neural Network (N.N.) with 1 hidden layer and 4 nodes
nn_1H_4N <- neuralnet(is_claim ~ .,
                    data = train_dummy_downsampled,
                    hidden = c(4)) # 1 hidden layer of 4 nodes
                    


plot(nn_1H_4N, rep="best") # plots the neural net with 4 nodes



# Predict the output on validation set
validation_prediction_1H_4N <- predict(nn_1H_4N,
                                      valid_dummy[,-16],
                                      type = "class")

validation_prediction_1H_4N_binary <-
  ifelse(validation_prediction_1H_4N[,1]
         >= 0.5, 1, 0) # Transform probabilities as binary outcome 
```
### Performance evaluation

```{r}


# Confusion matrix for classification tree with validation set
conf_matrix_1H2N_valid <- confusionMatrix(
 factor(validation_prediction_1H_2N_binary),
 factor(valid_dummy$is_claim), # change dataset
 positive = "1")

conf_matrix_1H2N_valid

# plot confusion matrix
fourfoldplot(conf_matrix_1H2N_valid$table,
            color = c("cyan", "pink"),
            conf.level = 0,
            margin = 1,
            main = "Confusion Matrix for Neural Network
            1 hidden layer of 2 nodes for validation set") 


# Confusion matrix for classification tree with validation set
conf_matrix_1H4N_valid <- confusionMatrix(
factor(validation_prediction_1H_4N_binary),
factor(valid_dummy$is_claim), # change dataset
positive = "1")

conf_matrix_1H4N_valid

# plot confusion matrix
fourfoldplot(conf_matrix_1H4N_valid$table,
           color = c("cyan", "pink"),
           conf.level = 0,
           margin = 1,
           main = "Confusion Matrix for Neural Network
           1 hidden layer of 4 nodes for validation set") 
```
Looking at the performance metrics for the neural network with 2 nodes:

-   Accuracy highs at 41.8 %

-   Specificity highs at 39.2 %

-   Sensitivity is 78.8%


Looking at the performance metrics for the neural network with 4 nodes:

-   Accuracy highs at 48.7 %

-   Specificity highs at 46.9 %

-   Sensitivity is 74.7%


From all positive outcome predicted, almost 8.8% (versus 8.1% for 2 nodes) where correctly predicted while almost 96.4% (versus 96.4 for 2 nodes) of negative outcomes where correctly predicted for the higher nodes neural network. Also, the performances for accuracy improved of almost 7% and the one for specificity improved of almost 8%, but the performance was lower for sensitivity of almost 4%.

### Conclusion on method performance
We conclude that predicting positive outcomes is still difficult since not even 10% of positive predictions are correctly predicted. Nonetheless, we see that a Neural Net method with one hidden layer and four nodes provides better results than a classification tree to reach our business goal.

## Logistic Regression

### Method Description

The logistic regression is a model that is also suited for binary response variables. It is a modeling technique used in statistics to predict the probability of an event occurring based on different independent variables. <br> Building the model with a logit link function as it is usually the best suited one for the logistic regression.

### Method Application

```{r}
cars.lg <- glm(is_claim ~., data= train_downsampling, family=binomial(link="logit"))

summary(cars.lg) # Displaying the resulting model
```

From the dummies born from "model" variable, only model =M2 is statistically significant. H0 cannot be rejected for the beta estimates of population_density and age_of_policeholder either. The other explanatory variables are significant, but the intercept is not. <br>

```{r}
PlotResidLogist(cars.lg) # Deviance residuals
```

If we plot the deviance residuals, we notice that the positive residuals and negative ones each form a clear group.<br> The residuals appearing only on one half of the observations is due to downsampling. The way every observation is tightly grouped may result from the artificially generated data. It is also the case for the pearson residuals. We can see an extreme residual that stands out (observation 440). Considering the amount of observations, this single observation should not affect the model too much. A robust logisitc regression is not required.

```{r}
anova(cars.lg, test="Chisq")
```

According to the analysis of the deviance table, "population_density","age_of_policyholder", and "model" could be unworth to be part of the model with p-value\>0.05. <br> To further investigate the importance of the variables, we can run a stepwise selection.

```{r}
cars_glm_back <- step(cars.lg, direction = "backward") # Model with backward selection
summary(cars_glm_back)
```

The backward selection results in a model with 3 explanatory variables: policy_tenure and age_of_car. Every beta is very stastically significant.

```{r}
cars_glm_forward <- step(cars.lg, direction = "forward") # Model with forward selection
summary(cars_glm_forward)
```

The forward selection ends up with different variables: the model is the same as the full one. This selection results in a minority of statistically significant beta estimates.

```{r}
cars_glm_both <- step(cars.lg, direction = "both") # Forward and backward
summary(cars_glm_both)
```

The selection with forward and backward at the same time results in the same variables as the backward selection. Every coefficient is also significant. The AIC of the reduced model is marginally better.

### Performance Evaluation

Confusion Matrix

```{r}
# Full model
pred_logi <- predict(cars.lg, newdata = cars_validation[,-16], type = "response")

# 50% cutoff
logi_outcome <- as.data.table(pred_logi)
logi_outcome <- ifelse(logi_outcome > 0.5, 1, 0)


#Confusion Matrix
cm1 <- confusionMatrix(
  factor(logi_outcome),
  factor(cars_validation$is_claim), 
  positive = "1")
#Plot
fourfoldplot(cm1$table,
             color = c("cyan", "pink"),
             conf.level = 0,
             margin = 1,
             main = "Confusion Matrix for the full logistic regression") 
```

The downsampling allows us to better detect when a claim occurs, but it is at the cost of increasing the false positives which are numerous as we can see. We can try to choose another cutoff to see if we can improve the model.

```{r}
# cutoff at probability of 0.55
logi_outcome <- as.data.table(pred_logi)
logi_outcome <- ifelse(logi_outcome > 0.55, 1, 0)

#Confusion Matrix
cm2 <- confusionMatrix(
  factor(logi_outcome),
  factor(cars_validation$is_claim), 
  positive = "1")
#Plot
fourfoldplot(cm2$table,
             color = c("cyan", "pink"),
             conf.level = 0,
             margin = 1,
             main = "Confusion Matrix for the full logistic regression") 
```

We can see the tradeoff right away. The 5% increased on the cutoff value reduced the predictions of is_claim by about 30% for both true positives and false positives.

```{r}
# Backward/Both 
cars.lg2 <- glm(is_claim ~ policy_tenure+age_of_car , data= train_downsampling, family=binomial(link="logit"))
pred_logi_back <- predict(cars.lg2, newdata = cars_validation[,-16], type = "response")

# 50% cutoff
logi_outcome_back <- as.data.table(pred_logi_back)
logi_outcome_back <- ifelse(logi_outcome_back > 0.5, 1, 0)

#Confusion Matrix
cm3 <- confusionMatrix(
  factor(logi_outcome_back),
  factor(cars_validation$is_claim), 
  positive = "1")
#Plot
fourfoldplot(cm3$table,
             color = c("cyan", "pink"),
             conf.level = 0,
             margin = 1,
             main = "Confusion Matrix for the reduced logistic regression") 
```

With the model with a reduced number of variables, we seem to get a little bit more false positives true positive: more is_claim= 1 predictions. The prediction performance is very similar, but one could argue that this reduced model performs marginally better since it reduces the false positives more than the true positives. Thus, to reduce computational resources and have a slightly better performing model, we can choose the model with the reduced number of variables. <br> <br> We can try to see whether the model can perform better with the whole training set combined with a very low cutoff value.

```{r}
# Accuracy
cm3$overall["Accuracy"]
#Sensitivity
cm3$byClass["Sensitivity"]
#Specificity
cm3$byClass["Specificity"]
```

The accuracy of 51.8% reflects the high number of false positives and negatives.

```{r}
# Backward/Both 
cars.lg3 <- glm(is_claim ~ policy_tenure+age_of_car , data= cars_train, family=binomial(link="logit"))
pred_logi_back2 <- predict(cars.lg3, newdata = cars_validation[,-16], type = "response")

# 6% cutoff
logi_outcome_back2 <- as.data.table(pred_logi_back2)
logi_outcome_back2 <- ifelse(logi_outcome_back2 > 0.06, 1, 0)

#Confusion Matrix
cm4 <- confusionMatrix(
  factor(logi_outcome_back2),
  factor(cars_validation$is_claim), 
  positive = "1")
#Plot
fourfoldplot(cm4$table,
             color = c("cyan", "pink"),
             conf.level = 0,
             margin = 1,
             main = "Confusion Matrix for the reduced logistic regression") 
```

```{r}
# Accuracy
cm4$overall["Accuracy"]
```

With a cutoff of 6%, the model performs similarly to the model constructed on the downsample. Decreasing of increasing the cutoff results in the same aforementioned tradeoff. We get similar performance with this model, but at very low cutoff values. At a 0.5 and even 0.2, the sensitivity/specificity tradeoff is maxed out: specificity is at 1 and sensitivity 0.

Lift Chart

```{r}
logi_outcome <- as.data.table(pred_logi_back)

liftChart(factor(logi_outcome$pred_logi_back) , factor(cars_validation$is_claim))
```

```{r}
topDecileLift(factor(logi_outcome$pred_logi) , factor(cars_validation$is_claim))
```

The top decile lift is at 1.63, and it gradually decreases in a linear fashion.

### Conclusion on method performance

The logistic regression's performance is not great. The accuracy is only at 51.8% for the chosen model (2 explanatory variables with downsample) and the sensitivity of 64.4% lets room for a lot of undetected claims. The false positives are also numerous with a specificity of 50.8%.

## Ensemble


### Methods's description

Machine learning ensemble approaches combine the results of various models to provide a single prediction. By avoiding overfitting and enhancing prediction robustness, ensemble approaches aim to enhance the generalization performance of the model.

### Methods's application


```{r}
#Actual outcome
actual_outcome <- cars_validation[,6]

# Classification Tree
tree.outcome2 <- cbind(as.data.frame(tree.outcome),tree_prob=tree.outcome)
tree.outcome2$tree.outcome <- ifelse(tree.outcome2$tree.outcome>0.5,1,0)
colnames(tree.outcome2) <- c("tree_pred","tree_prob")

#KNN
knn_outcome2 <- as.data.frame(knn_outcome)
colnames(knn_outcome2) <- c("knn_pred", "knn_prob")

#Neural Network
nn_outcome2 <- data.frame(validation_prediction_1H_4N[,1])
nn_outcome2 <- cbind(ifelse(nn_outcome2>=0.5, 1, 0), nn_outcome2)
colnames(nn_outcome2) <- c("nn_pred","nn_prob")

#Logistic Regression
logi_outcome2 <- cbind( logi_outcome, logi_outcome)
colnames(logi_outcome2) <- c("logi_pred","logi_prob")
logi_outcome2$logi_pred <- ifelse(logi_outcome2$logi_pred>0.5,1,0)

ensemble <- data.table(actual_outcome,tree.outcome2,knn_outcome2,nn_outcome2,logi_outcome2)

```


```{r}
# Average probabilities column
ensemble[, avg_prob := (logi_prob + knn_prob + tree_prob + nn_prob) / 4]

# # Majority vote
ensemble[, maj_vote := as.numeric(as.character(logi_pred))+ as.numeric(as.character(knn_pred)) + as.numeric(as.character(tree_pred))+ as.numeric(as.character(nn_pred))]

ensemble[, maj_vote := ifelse(maj_vote >= 3, 1, 0)]

# Display first 10 rows
kable(ensemble[1:10, ], format = "markdown")
```

### Performance Evaluation

Using the average probability with a 0.5 cutoff
```{r}
#Confusion Matrix 
cm_ens_prob <- confusionMatrix(
  factor(ifelse(ensemble$avg_prob > 0.5, 1 ,0)),
  factor(cars_validation$is_claim), 
  positive = "1")
#Plot
fourfoldplot(cm_ens_prob$table,
             color = c("cyan", "pink"),
             conf.level = 0,
             margin = 1,
             main = "Confusion Matrix Ensemble Average Probability") 
```

```{r}
# Accuracy
cm_ens_prob$overall["Accuracy"]
#Sensitivity
cm_ens_prob$byClass["Sensitivity"]
#Specificity
cm_ens_prob$byClass["Specificity"]
```



Using the majority vote
```{r}
cm_ens_maj <- confusionMatrix(
  factor(ensemble$maj_vote),
  factor(cars_validation$is_claim), 
  positive = "1")
#Plot
fourfoldplot(cm_ens_maj$table,
             color = c("cyan", "pink"),
             conf.level = 0,
             margin = 1,
             main = "Confusion Matrix Ensemble Majority vote") 
```

```{r}
# Accuracy
cm_ens_maj$overall["Accuracy"]
#Sensitivity
cm_ens_maj$byClass["Sensitivity"]
#Specificity
cm_ens_maj$byClass["Specificity"]
```

The majority vote approach's lesser sensitivity may be due in part to the fact that its projections are more "moderate" in terms of is_claim=1 predictions. Making a choice based on the majority vote effectively requires that the majority of the models agree on the forecast, which can reduce the likelihood of false positives (i.e., predicting the outcome variable as 1 when it is actually 0). Because there will be fewer overall true positive predictions, the sensitivity may be reduced as a result.
<br>
<br>
On the other hand, because it is essentially averaging the probabilities of all the models, the average probability technique may be less conservative in its forecasts (more is_claim =1 predictions). Although there may be more false positives as a result, there may also be more actual positive predictions, increasing sensitivity.

### Conclusion

Overall, given that the majority vote strategy offers greater precision and specificity than the average probability approach, it would be a preferable option for this particular case. <br>
When selecting an ensemble approach, it's crucial to take the trade-offs between the various performance measures into account because different applications could give particular metrics a higher priority (e.g., sensitivity versus specificity).


# All methods comparison

```{r}
Accuracy <-  data.frame(LR = round(cm3$overall[1],3))  #predict accuracy by using logistic regression 
Accuracy <- cbind(Accuracy, KNN = round(cmk$overall[1],3))      #predict accuracy by using KNN
Accuracy <- cbind(Accuracy,CT = round(conf_matrix_tree_v$overall[1],3))    #predict accuracy by using tree
Accuracy <- cbind(Accuracy,NN=round(conf_matrix_1H2N_valid$overall[1],3))     #predict accuracy by using neural network
Accuracy.melt = melt(Accuracy)
ggplot(data = Accuracy.melt,aes(x=reorder(variable, value),y=value))+ylab("Accuracy") + xlab("Method") +
  geom_bar(stat="identity", fill="steelblue")+
  geom_text(aes(label=value), vjust=1.6, color="white", size=3.5)+
  theme_minimal()
```

```{r}
Specificity_P <-  data.frame(LR = round(cm2$byClass[2],3))  #predict specificity by using logistic regression 
Specificity_P <- cbind(Specificity_P, KNN = round(cmk$byClass[2],3))       #predict specificity by using KNN
Specificity_P <- cbind(Specificity_P,CT = round(conf_matrix_tree_v$byClass[2],3))     #predicts specificity by using tree
Specificity_P <- cbind(Specificity_P,NN=round(conf_matrix_1H2N_valid$byClass[2],3))    #predict specificity by using neural network
Specificity_P.melt = melt(Specificity_P)
ggplot(data = Specificity_P.melt,aes(x=reorder(variable, value),y=value))+ylab("Specificity") + xlab("Method") +
  geom_bar(stat="identity", fill="steelblue")+
  geom_text(aes(label=value), vjust=1.6, color="white", size=3.5)+
  theme_minimal()

```

```{r}
Sensitivity <-  data.frame(LR = round(cm2$byClass[1],3))  #predict Sensitivity by using logistic regression 
Sensitivity <- cbind(Sensitivity, KNN = round(cmk$byClass[1],3))       #predict Sensitivity by using KNN
Sensitivity <- cbind(Sensitivity,CT = round(conf_matrix_tree_v$byClass[1],3))     #predicts Sensitivity by using tree
Sensitivity <- cbind(Sensitivity,NN=round(conf_matrix_1H2N_valid$byClass[1],3))    #predict Sensitivity by using neural network
Sensitivity.melt = melt(Sensitivity)
ggplot(data = Sensitivity.melt,aes(x=reorder(variable, value),y=value))+ylab("Sensitivity") + xlab("Method") +
  geom_bar(stat="identity", fill="steelblue")+
  geom_text(aes(label=value), vjust=1.6, color="white", size=3.5)+
  theme_minimal()

```

# Conclusions
